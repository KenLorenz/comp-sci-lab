{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Documentation\n",
    "Overview\n",
    "\n",
    "This documentation provides details on a handwritten character recognition model developed using TensorFlow and Keras. The project is split into three main scripts: create_model.py, dataset.py, and improve_model.py. The purpose of the model is to recognize characters from the A-Z Handwritten Data dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. dataset.py\n",
    "\n",
    "1.1. split_dataset\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Splits the A-Z Handwritten Data dataset into training and testing sets.\n",
    "\n",
    "Functionality\n",
    "Reads the dataset from 'main_dataset/A_Z Handwritten Data.csv'.\n",
    "Splits the dataset into features (x) and labels (y).\n",
    "Performs a train-test split (80% training, 20% testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(): # returns in-order: x_train, x_test, y_train, y_test\n",
    "    csv_file= pd.read_csv('main_dataset/A_Z Handwritten Data.csv').astype('float32')\n",
    "        \n",
    "    dataset = pd.DataFrame(csv_file)\n",
    "    \n",
    "    x = dataset.drop('0', axis = 1)\n",
    "    y = dataset['0']\n",
    "    \n",
    "    return train_test_split(x, y, test_size = 0.2) # 20% train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. dataset_train_test_to_csv\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Creates CSV files for training and testing sets.\n",
    "\n",
    "Functionality\n",
    "Calls split_dataset to get x_train, y_train, x_test, y_test.\n",
    "Creates CSV files for each set in the 'subdataset' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_csv(x1,y1,x2,y2): # creates csv for x_train, x_test, y_train, y_test\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = split_dataset()\n",
    "    \n",
    "    print('\\nCreating Csv 1/4')\n",
    "    x_train.head(x1).to_csv('subdataset/x_train.csv', index=False, header=False)\n",
    "    \n",
    "    print('\\nCreating Csv 2/4')\n",
    "    y_train.head(y1).to_csv('subdataset/y_train.csv', index=False, header=False)\n",
    "    \n",
    "    print('\\nCreating Csv 3/4')\n",
    "    x_test.head(x2).to_csv('subdataset/x_test.csv', index=False, header=False)\n",
    "    \n",
    "    print('\\nCreating Csv 4/4')\n",
    "    y_test.head(y2).to_csv('subdataset/y_test.csv', index=False, header=False)\n",
    "    \n",
    "    print('\\n-- Done --')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. verify_all_datasets\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Loads all datasets and prints their shapes.\n",
    "\n",
    "Functionality\n",
    "Calls dataset loading functions and prints shapes for x_train, y_train, x_test, and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_all_datasets(): # loads all datasets, still kept just in-case.\n",
    "    \n",
    "    x_train = load_x_train()\n",
    "    y_train = load_y_train()\n",
    "    x_test= load_x_test()\n",
    "    y_test = load_y_test()\n",
    "    \n",
    "    print(f'\\nx_train shape: {x_train.shape}')\n",
    "    print(f'\\ny_train shape: {y_train.shape}')\n",
    "    print(f'\\nx_test shape: {x_test.shape}')\n",
    "    print(f'\\ny_test shape: {y_test.shape}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4. load_x_train, load_y_train, load_x_test, load_y_test\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Loads specific datasets from CSV files.\n",
    "\n",
    "Functionality\n",
    "Reads the corresponding CSV files and returns the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_x_train():\n",
    "    return pd.read_csv('subdataset/x_train.csv', header=None).astype('float32')\n",
    "\n",
    "def load_y_train():\n",
    "    return pd.read_csv('subdataset/y_train.csv', header=None).astype('float32')\n",
    "\n",
    "def load_x_test():\n",
    "    return pd.read_csv('subdataset/x_test.csv', header=None).astype('float32')\n",
    "\n",
    "def load_y_test():\n",
    "    return pd.read_csv('subdataset/y_test.csv', header=None).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5. limit_train_count\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Limits the number of training iterations based on user input.\n",
    "\n",
    "Functionality\n",
    "Takes a user-input count and ensures it is within specified bounds (min: 0, max: 10,000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_train_count(trainCount, min, max):\n",
    "    if(trainCount > max): return max\n",
    "    elif(trainCount < min): return min\n",
    "    return trainCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. create_model.py\n",
    "\n",
    "2.1. Script Overview\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Creates and trains a handwritten character recognition model using a specified architecture.\n",
    "\n",
    "Key Components:\n",
    "\n",
    "Sequential model with input layer, two hidden layers, and output layer.\n",
    "Dense layers with ReLU activation and L2 regularization.\n",
    "Training with Adam optimizer and Sparse Categorical Crossentropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 14:21:27.070613: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-13 14:21:27.142181: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-13 14:21:27.737723: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-13 14:21:27.741317: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-13 14:21:28.977719: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf;\n",
    "from dataset import load_x_train, load_y_train, limit_train_count\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Script Flow\n",
    "\n",
    "Loading Dataset\n",
    "\n",
    "Calls load_x_train and load_y_train to load training data.\n",
    "\n",
    "Creating Model\n",
    "\n",
    "    -Defines a Sequential model with input layer, two hidden layers, and output layer.\n",
    "    -Uses ReLU activation and L2 regularization in hidden layers.\n",
    "\n",
    "Training Model\n",
    "\n",
    "    -Compiles the model with Adam optimizer and Sparse Categorical Crossentropy loss.\n",
    "    -Takes user input for initial training iterations.\n",
    "    -Fits the model to the training data with validation split.\n",
    "\n",
    "Saving Model\n",
    "\n",
    "    -Takes user input for a new model name.\n",
    "    -Saves the trained model in the 'model' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Loading dataset...\n",
      "\n",
      "x_train shape: (20000, 784)\n",
      "\n",
      "y_train shape: (20000, 1)\n",
      "\n",
      "-- Dataset prepared!\n",
      "\n",
      "-- Creating Model\n",
      "\n",
      "-- Model Created!\n",
      "\n",
      "-- Training model\n",
      "\n",
      "Epoch 1/100\n",
      "563/563 [==============================] - 2s 3ms/step - loss: 65.5326 - accuracy: 0.2448 - val_loss: 41.0778 - val_accuracy: 0.2835\n",
      "Epoch 2/100\n",
      "563/563 [==============================] - 1s 3ms/step - loss: 36.9336 - accuracy: 0.3060 - val_loss: 33.6931 - val_accuracy: 0.3535\n",
      "Epoch 3/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 30.9811 - accuracy: 0.3902 - val_loss: 28.3268 - val_accuracy: 0.4455\n",
      "Epoch 4/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 25.7767 - accuracy: 0.4799 - val_loss: 23.2422 - val_accuracy: 0.5400\n",
      "Epoch 5/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 20.8853 - accuracy: 0.5423 - val_loss: 18.5777 - val_accuracy: 0.5735\n",
      "Epoch 6/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 16.4867 - accuracy: 0.5888 - val_loss: 14.4754 - val_accuracy: 0.6165\n",
      "Epoch 7/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 12.6931 - accuracy: 0.6222 - val_loss: 11.0397 - val_accuracy: 0.6440\n",
      "Epoch 8/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 9.5579 - accuracy: 0.6589 - val_loss: 8.2820 - val_accuracy: 0.6780\n",
      "Epoch 9/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 7.0816 - accuracy: 0.7014 - val_loss: 6.1193 - val_accuracy: 0.7145\n",
      "Epoch 10/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 5.2259 - accuracy: 0.7377 - val_loss: 4.5340 - val_accuracy: 0.7515\n",
      "Epoch 11/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 3.8486 - accuracy: 0.7895 - val_loss: 3.4234 - val_accuracy: 0.7865\n",
      "Epoch 12/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 2.9003 - accuracy: 0.8305 - val_loss: 2.6207 - val_accuracy: 0.8305\n",
      "Epoch 13/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 2.2610 - accuracy: 0.8561 - val_loss: 2.1286 - val_accuracy: 0.8490\n",
      "Epoch 14/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 1.8157 - accuracy: 0.8734 - val_loss: 1.7544 - val_accuracy: 0.8530\n",
      "Epoch 15/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 1.4969 - accuracy: 0.8918 - val_loss: 1.4999 - val_accuracy: 0.8670\n",
      "Epoch 16/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 1.2694 - accuracy: 0.9027 - val_loss: 1.3036 - val_accuracy: 0.8720\n",
      "Epoch 17/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 1.1003 - accuracy: 0.9108 - val_loss: 1.1150 - val_accuracy: 0.8930\n",
      "Epoch 18/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.9667 - accuracy: 0.9182 - val_loss: 1.0066 - val_accuracy: 0.8950\n",
      "Epoch 19/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.8603 - accuracy: 0.9223 - val_loss: 0.9159 - val_accuracy: 0.8885\n",
      "Epoch 20/100\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.7762 - accuracy: 0.9263 - val_loss: 0.8274 - val_accuracy: 0.9070\n",
      "Epoch 21/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.7049 - accuracy: 0.9326 - val_loss: 0.7671 - val_accuracy: 0.9090\n",
      "Epoch 22/100\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.6498 - accuracy: 0.9359 - val_loss: 0.7189 - val_accuracy: 0.9090\n",
      "Epoch 23/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.6068 - accuracy: 0.9389 - val_loss: 0.6620 - val_accuracy: 0.9195\n",
      "Epoch 24/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.5696 - accuracy: 0.9418 - val_loss: 0.6547 - val_accuracy: 0.9110\n",
      "Epoch 25/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.5399 - accuracy: 0.9424 - val_loss: 0.6277 - val_accuracy: 0.9130\n",
      "Epoch 26/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.5173 - accuracy: 0.9413 - val_loss: 0.5934 - val_accuracy: 0.9150\n",
      "Epoch 27/100\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.4945 - accuracy: 0.9434 - val_loss: 0.5841 - val_accuracy: 0.9120\n",
      "Epoch 28/100\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.4772 - accuracy: 0.9457 - val_loss: 0.5471 - val_accuracy: 0.9305\n",
      "Epoch 29/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.4622 - accuracy: 0.9486 - val_loss: 0.5559 - val_accuracy: 0.9200\n",
      "Epoch 30/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.4527 - accuracy: 0.9463 - val_loss: 0.5355 - val_accuracy: 0.9255\n",
      "Epoch 31/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.4391 - accuracy: 0.9484 - val_loss: 0.5095 - val_accuracy: 0.9280\n",
      "Epoch 32/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.4325 - accuracy: 0.9486 - val_loss: 0.4982 - val_accuracy: 0.9375\n",
      "Epoch 33/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.4204 - accuracy: 0.9505 - val_loss: 0.5030 - val_accuracy: 0.9295\n",
      "Epoch 34/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.4170 - accuracy: 0.9515 - val_loss: 0.4994 - val_accuracy: 0.9290\n",
      "Epoch 35/100\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.4104 - accuracy: 0.9504 - val_loss: 0.4873 - val_accuracy: 0.9290\n",
      "Epoch 36/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.4052 - accuracy: 0.9525 - val_loss: 0.4676 - val_accuracy: 0.9315\n",
      "Epoch 37/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.3963 - accuracy: 0.9534 - val_loss: 0.4802 - val_accuracy: 0.9325\n",
      "Epoch 38/100\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.3982 - accuracy: 0.9518 - val_loss: 0.4800 - val_accuracy: 0.9235\n",
      "Epoch 39/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.3904 - accuracy: 0.9549 - val_loss: 0.4863 - val_accuracy: 0.9265\n",
      "Epoch 40/100\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.3824 - accuracy: 0.9557 - val_loss: 0.4982 - val_accuracy: 0.9200\n",
      "Epoch 41/100\n",
      "563/563 [==============================] - 1s 1ms/step - loss: 0.3814 - accuracy: 0.9531 - val_loss: 0.4865 - val_accuracy: 0.9230\n",
      "Epoch 42/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.3745 - accuracy: 0.9562 - val_loss: 0.4645 - val_accuracy: 0.9300\n",
      "Epoch 43/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.3687 - accuracy: 0.9566 - val_loss: 0.4567 - val_accuracy: 0.9255\n",
      "Epoch 44/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.3678 - accuracy: 0.9569 - val_loss: 0.4532 - val_accuracy: 0.9250\n",
      "Epoch 45/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.3646 - accuracy: 0.9573 - val_loss: 0.4459 - val_accuracy: 0.9350\n",
      "Epoch 46/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.3612 - accuracy: 0.9588 - val_loss: 0.4446 - val_accuracy: 0.9355\n",
      "Epoch 47/100\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 0.3589 - accuracy: 0.9583 - val_loss: 0.4730 - val_accuracy: 0.9180\n",
      "Epoch 48/100\n",
      " 32/563 [>.............................] - ETA: 0s - loss: 0.3569 - accuracy: 0.9600"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ren/Documents/comp-sci/gitfiles/comp-sci-lab/model_analysis.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ren/Documents/comp-sci/gitfiles/comp-sci-lab/model_analysis.ipynb#X21sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m trainCount \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39minput\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mInitial Training Iterations (min: 1, max: 1000): \u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ren/Documents/comp-sci/gitfiles/comp-sci-lab/model_analysis.ipynb#X21sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ren/Documents/comp-sci/gitfiles/comp-sci-lab/model_analysis.ipynb#X21sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mSparseCategoricalCrossentropy(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ren/Documents/comp-sci/gitfiles/comp-sci-lab/model_analysis.ipynb#X21sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ren/Documents/comp-sci/gitfiles/comp-sci-lab/model_analysis.ipynb#X21sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ren/Documents/comp-sci/gitfiles/comp-sci-lab/model_analysis.ipynb#X21sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ren/Documents/comp-sci/gitfiles/comp-sci-lab/model_analysis.ipynb#X21sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, validation_split \u001b[39m=\u001b[39;49m \u001b[39m0.1\u001b[39;49m, epochs\u001b[39m=\u001b[39;49mlimit_train_count(trainCount,\u001b[39m0\u001b[39;49m,\u001b[39m1000\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ren/Documents/comp-sci/gitfiles/comp-sci-lab/model_analysis.ipynb#X21sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m-- Training end, saving model\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ren/Documents/comp-sci/gitfiles/comp-sci-lab/model_analysis.ipynb#X21sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m modelName \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39minput\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEnter a new model name: \u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m~/Documents/comp-sci/gitfiles/comp-sci-lab/.venv/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Documents/comp-sci/gitfiles/comp-sci-lab/.venv/lib/python3.8/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Documents/comp-sci/gitfiles/comp-sci-lab/.venv/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Documents/comp-sci/gitfiles/comp-sci-lab/.venv/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Documents/comp-sci/gitfiles/comp-sci-lab/.venv/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Documents/comp-sci/gitfiles/comp-sci-lab/.venv/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:149\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m    148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39m_call_flat(\n\u001b[0;32m--> 149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/Documents/comp-sci/gitfiles/comp-sci-lab/.venv/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1535\u001b[0m, in \u001b[0;36mConcreteFunction.captured_inputs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m   1529\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcaptured_inputs\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1530\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns external Tensors captured by this function.\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \n\u001b[1;32m   1532\u001b[0m \u001b[39m  self.__call__(*args) passes `args + self.captured_inputs` to the function.\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m   1534\u001b[0m   \u001b[39mreturn\u001b[39;00m nest\u001b[39m.\u001b[39mflatten(\n\u001b[0;32m-> 1535\u001b[0m       [x() \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(x) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_captured_inputs],\n\u001b[1;32m   1536\u001b[0m       expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/comp-sci/gitfiles/comp-sci-lab/.venv/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1535\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m   1529\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcaptured_inputs\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1530\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns external Tensors captured by this function.\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \n\u001b[1;32m   1532\u001b[0m \u001b[39m  self.__call__(*args) passes `args + self.captured_inputs` to the function.\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m   1534\u001b[0m   \u001b[39mreturn\u001b[39;00m nest\u001b[39m.\u001b[39mflatten(\n\u001b[0;32m-> 1535\u001b[0m       [x() \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(x) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_captured_inputs],\n\u001b[1;32m   1536\u001b[0m       expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('-- Loading dataset...')\n",
    "\n",
    "x_train = load_x_train()\n",
    "y_train = load_y_train()\n",
    "\n",
    "print(f'\\nx_train shape: {x_train.shape}')\n",
    "print(f'\\ny_train shape: {y_train.shape}')\n",
    "\n",
    "print('\\n-- Dataset prepared!')\n",
    "\n",
    "print('\\n-- Creating Model')\n",
    "\n",
    "model = Sequential(\n",
    "    [               \n",
    "        tf.keras.Input(shape=(784,)),\n",
    "        \n",
    "        Dense(126, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.2)),\n",
    "        Dense(64, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.2)),\n",
    "        Dense(26, activation='softmax')\n",
    "        \n",
    "    ], name = \"new_model\" \n",
    ") # recommended fit = 200\n",
    "\n",
    "print('\\n-- Model Created!')\n",
    "\n",
    "print('\\n-- Training model\\n')\n",
    "\n",
    "trainCount = int(input('Initial Training Iterations (min: 1, max: 1000): '))\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, validation_split = 0.1, epochs=limit_train_count(trainCount,0,1000))\n",
    "\n",
    "\n",
    "print('\\n-- Training end, saving model\\n')\n",
    "modelName = str(input('Enter a new model name: '))\n",
    "model.save(f'./model/{modelName}.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. improve_model.py\n",
    "\n",
    "3.1. Script Overview\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Loads an existing model and performs additional training to improve performance.\n",
    "\n",
    "Key Components:\n",
    "\n",
    "Loads a pre-existing model using tf.keras.models.load_model.\n",
    "Takes user input for additional training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf;\n",
    "from dataset import load_x_train, load_y_train, limit_train_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2. Script Flow\n",
    "\n",
    "Loading Dataset\n",
    "\n",
    "    -Calls load_x_train and load_y_train to load training data.\n",
    "\n",
    "Loading Existing Model\n",
    "\n",
    "    -Takes user input for an existing model name.\n",
    "    -Attempts to load the model; exits if not found.\n",
    "\n",
    "Additional Training\n",
    "\n",
    "    -Takes user input for additional training iterations.\n",
    "    -Performs additional training on the loaded model.\n",
    "\n",
    "Updating and Saving Model\n",
    "\n",
    "    -Takes user input for a new model name.\n",
    "    -Saves the updated model in the 'model' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- Loading dataset...')\n",
    "\n",
    "x_train = load_x_train()\n",
    "y_train = load_y_train()\n",
    "\n",
    "print(f'\\nx_train shape: {x_train.shape}')\n",
    "print(f'\\ny_train shape: {y_train.shape}')\n",
    "\n",
    "print('\\n-- Dataset prepared!')\n",
    "\n",
    "print('\\n-- Training model\\n') \n",
    "\n",
    "\n",
    "modelName = str(input('Enter an existing model name: '))\n",
    "try:\n",
    "    modelLoad = tf.keras.models.load_model(f'model/{modelName}.keras')\n",
    "except:\n",
    "    print('Model not found!')\n",
    "    exit()\n",
    "\n",
    "trainCount = int(input('Training Iterations (min: 0, max: 1000): '))\n",
    "\n",
    "modelLoad.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "modelLoad.fit(\n",
    "    x_train, y_train, validation_split = 0.1,\n",
    "    epochs=limit_train_count(trainCount,0,1000) # basically an input with min and max limit\n",
    ")\n",
    "\n",
    "\n",
    "print('\\n-- Training end, updating model\\n')\n",
    "modelLoad.save(f'./model/{modelName}.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "The developed model provides a framework for character recognition, and the scripts facilitate dataset handling, model creation, and improvement. Users can leverage these scripts to experiment with different architectures, training iterations, and datasets for their specific requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
