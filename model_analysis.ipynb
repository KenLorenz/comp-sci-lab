{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Documentation\n",
    "Overview\n",
    "\n",
    "This documentation provides details on a handwritten character recognition model developed using TensorFlow and Keras. The project is split into three main scripts: create_model.py, dataset.py, and improve_model.py. The purpose of the model is to recognize characters from the A-Z Handwritten Data dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. dataset.py\n",
    "\n",
    "1.1. split_dataset\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Splits the A-Z Handwritten Data dataset into training and testing sets.\n",
    "\n",
    "Functionality\n",
    "Reads the dataset from 'main_dataset/A_Z Handwritten Data.csv'.\n",
    "Splits the dataset into features (x) and labels (y).\n",
    "Performs a train-test split (80% training, 20% testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(): # returns in-order: x_train, x_test, y_train, y_test\n",
    "    csv_file= pd.read_csv('main_dataset/A_Z Handwritten Data.csv').astype('float32')\n",
    "        \n",
    "    dataset = pd.DataFrame(csv_file)\n",
    "    \n",
    "    x = dataset.drop('0', axis = 1)\n",
    "    y = dataset['0']\n",
    "    \n",
    "    return train_test_split(x, y, test_size = 0.2) # 20% train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. dataset_train_test_to_csv\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Creates CSV files for training and testing sets.\n",
    "\n",
    "Functionality\n",
    "Calls split_dataset to get x_train, y_train, x_test, y_test.\n",
    "Creates CSV files for each set in the 'subdataset' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_csv(x1,y1,x2,y2): # creates csv for x_train, x_test, y_train, y_test\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = split_dataset()\n",
    "    \n",
    "    print('\\nCreating Csv 1/4')\n",
    "    x_train.head(x1).to_csv('subdataset/x_train.csv', index=False, header=False)\n",
    "    \n",
    "    print('\\nCreating Csv 2/4')\n",
    "    y_train.head(y1).to_csv('subdataset/y_train.csv', index=False, header=False)\n",
    "    \n",
    "    print('\\nCreating Csv 3/4')\n",
    "    x_test.head(x2).to_csv('subdataset/x_test.csv', index=False, header=False)\n",
    "    \n",
    "    print('\\nCreating Csv 4/4')\n",
    "    y_test.head(y2).to_csv('subdataset/y_test.csv', index=False, header=False)\n",
    "    \n",
    "    print('\\n-- Done --')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. verify_all_datasets\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Loads all datasets and prints their shapes.\n",
    "\n",
    "Functionality\n",
    "Calls dataset loading functions and prints shapes for x_train, y_train, x_test, and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_all_datasets(): # loads all datasets, still kept just in-case.\n",
    "    \n",
    "    x_train = load_x_train()\n",
    "    y_train = load_y_train()\n",
    "    x_test= load_x_test()\n",
    "    y_test = load_y_test()\n",
    "    \n",
    "    print(f'\\nx_train shape: {x_train.shape}')\n",
    "    print(f'\\ny_train shape: {y_train.shape}')\n",
    "    print(f'\\nx_test shape: {x_test.shape}')\n",
    "    print(f'\\ny_test shape: {y_test.shape}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4. load_x_train, load_y_train, load_x_test, load_y_test\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Loads specific datasets from CSV files.\n",
    "\n",
    "Functionality\n",
    "Reads the corresponding CSV files and returns the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_x_train():\n",
    "    return pd.read_csv('subdataset/x_train.csv', header=None).astype('float32')\n",
    "\n",
    "def load_y_train():\n",
    "    return pd.read_csv('subdataset/y_train.csv', header=None).astype('float32')\n",
    "\n",
    "def load_x_test():\n",
    "    return pd.read_csv('subdataset/x_test.csv', header=None).astype('float32')\n",
    "\n",
    "def load_y_test():\n",
    "    return pd.read_csv('subdataset/y_test.csv', header=None).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5. limit_train_count\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Limits the number of training iterations based on user input.\n",
    "\n",
    "Functionality\n",
    "Takes a user-input count and ensures it is within specified bounds (min: 0, max: 10,000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_train_count(trainCount, min, max):\n",
    "    if(trainCount > max): return max\n",
    "    elif(trainCount < min): return min\n",
    "    return trainCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. create_model.py\n",
    "\n",
    "2.1. Script Overview\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Creates and trains a handwritten character recognition model using a specified architecture.\n",
    "\n",
    "Key Components:\n",
    "\n",
    "Sequential model with input layer, two hidden layers, and output layer.\n",
    "Dense layers with ReLU activation and L2 regularization.\n",
    "Training with Adam optimizer and Sparse Categorical Crossentropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf;\n",
    "from dataset import load_x_train, load_y_train, limit_train_count\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Script Flow\n",
    "\n",
    "Loading Dataset\n",
    "\n",
    "Calls load_x_train and load_y_train to load training data.\n",
    "\n",
    "Creating Model\n",
    "\n",
    "    -Defines a Sequential model with input layer, two hidden layers, and output layer.\n",
    "    -Uses ReLU activation and L2 regularization in hidden layers.\n",
    "\n",
    "Training Model\n",
    "\n",
    "    -Compiles the model with Adam optimizer and Sparse Categorical Crossentropy loss.\n",
    "    -Takes user input for initial training iterations.\n",
    "    -Fits the model to the training data with validation split.\n",
    "\n",
    "Saving Model\n",
    "\n",
    "    -Takes user input for a new model name.\n",
    "    -Saves the trained model in the 'model' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- Loading dataset...')\n",
    "\n",
    "x_train = load_x_train()\n",
    "y_train = load_y_train()\n",
    "\n",
    "print(f'\\nx_train shape: {x_train.shape}')\n",
    "print(f'\\ny_train shape: {y_train.shape}')\n",
    "\n",
    "print('\\n-- Dataset prepared!')\n",
    "\n",
    "print('\\n-- Creating Model')\n",
    "\n",
    "model = Sequential(\n",
    "    [               \n",
    "        tf.keras.Input(shape=(784,)),\n",
    "        \n",
    "        Dense(126, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.2)),\n",
    "        Dense(64, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.2)),\n",
    "        Dense(26, activation='softmax')\n",
    "        \n",
    "    ], name = \"new_model\" \n",
    ") # recommended fit = 200\n",
    "\n",
    "print('\\n-- Model Created!')\n",
    "\n",
    "print('\\n-- Training model\\n')\n",
    "\n",
    "trainCount = int(input('Initial Training Iterations (min: 1, max: 1000): '))\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, validation_split = 0.1, epoch=limit_train_count(trainCount,0,1000))\n",
    "\n",
    "\n",
    "print('\\n-- Training end, saving model\\n')\n",
    "modelName = str(input('Enter a new model name: '))\n",
    "model.save(f'./model/{modelName}.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. improve_model.py\n",
    "\n",
    "3.1. Script Overview\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Loads an existing model and performs additional training to improve performance.\n",
    "\n",
    "Key Components:\n",
    "\n",
    "Loads a pre-existing model using tf.keras.models.load_model.\n",
    "Takes user input for additional training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf;\n",
    "from dataset import load_x_train, load_y_train, limit_train_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2. Script Flow\n",
    "\n",
    "Loading Dataset\n",
    "\n",
    "    -Calls load_x_train and load_y_train to load training data.\n",
    "\n",
    "Loading Existing Model\n",
    "\n",
    "    -Takes user input for an existing model name.\n",
    "    -Attempts to load the model; exits if not found.\n",
    "\n",
    "Additional Training\n",
    "\n",
    "    -Takes user input for additional training iterations.\n",
    "    -Performs additional training on the loaded model.\n",
    "\n",
    "Updating and Saving Model\n",
    "\n",
    "    -Takes user input for a new model name.\n",
    "    -Saves the updated model in the 'model' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-- Loading dataset...')\n",
    "\n",
    "x_train = load_x_train()\n",
    "y_train = load_y_train()\n",
    "\n",
    "print(f'\\nx_train shape: {x_train.shape}')\n",
    "print(f'\\ny_train shape: {y_train.shape}')\n",
    "\n",
    "print('\\n-- Dataset prepared!')\n",
    "\n",
    "print('\\n-- Training model\\n') \n",
    "\n",
    "\n",
    "modelName = str(input('Enter an existing model name: '))\n",
    "try:\n",
    "    modelLoad = tf.keras.models.load_model(f'model/{modelName}.keras')\n",
    "except:\n",
    "    print('Model not found!')\n",
    "    exit()\n",
    "\n",
    "trainCount = int(input('Training Iterations (min: 0, max: 1000): '))\n",
    "\n",
    "modelLoad.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "modelLoad.fit(\n",
    "    x_train, y_train, validation_split = 0.1,\n",
    "    epochs=limit_train_count(trainCount,0,1000) # basically an input with min and max limit\n",
    ")\n",
    "\n",
    "\n",
    "print('\\n-- Training end, updating model\\n')\n",
    "modelLoad.save(f'./model/{modelName}.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "The developed model provides a framework for character recognition, and the scripts facilitate dataset handling, model creation, and improvement. Users can leverage these scripts to experiment with different architectures, training iterations, and datasets for their specific requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
